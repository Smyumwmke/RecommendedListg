深度学习轻松学——核心算法与视觉实践 PDF下载 冯超 百度云 电子书 下载 电子书下载
PDF电子书下载不求人，看这篇文章就够了→ http://www.chendianrong.com/pdf#712131713
PDF电子书下载不求人，看这篇文章就够了→ http://www.chendianrong.com/pdf#712131713
<p>书名:深度学习轻松学——核心算法与视觉实践</p><p>作者:冯超</p><p>页数:334</p><p>定价:¥79.0</p><p>出版社:电子工业出版社</p><p>出版日期:2017-07-01</p><p>ISBN:9787121317132</p><p><h2>相关资料</h2></p>[<p>由深度学习引发的新一轮人工智能革命已经在众多领域颠覆了人们的认知，越来越多的人加入研究深度学习的大军。本书详尽介绍了深度学习的基本知识，以及视觉领域部分前沿应用，同时深入分析了工业界十分成熟的开源框架Caffe，可以帮助读者更快地夯实深度学习基础，跟上深度学习发展的前沿。作者行文在细节上十分认真，书中内容可读性很强，非常适合入门者阅读。
——
猿辅导研究总监，邓澍军
 
近年来，深度学习技术已经给学术界、工业界带来了极大的影响，本书深入浅出地介绍了深度学习基础知识与视觉应用，语言轻松幽默但不失严谨，内容既涵盖经典概念，又包括一些*的研究成果，特别是对一些底层的具体计算方式有细致的描述，这往往是深度学习入门者忽略的，因此非常适合深度学习的初学者和进阶者阅读学习。
——
今日头条 AI Lab 科学家，《推荐系统实践》作者，项亮
 
随着GPU、TPU 等专用处理芯片的发展，深度学习技术逐渐从幕后走向台前，开始向世人展现其强大的非线性映射能力。本书从神经网络的基础结构入手，深入分析了深度学习模型内部的算法细节，并总结近年来一些优秀的研究成果，非常适合有志于研究深度学习的初学者和希望快速了解深度学习基础知识与发展的研究人员阅读。
——
中国科学院计算技术研究所副研究员，刘淘英</p>]<p><h2>本书特色</h2></p>[<p>
《深度学习轻松学：核心算法与视觉实践》介绍了深度学习基本算法和视觉领域的应用实例。书中以轻松直白的语言，生动详细地介绍了深层模型相关的基础知识，并深入剖析了算法的原理与本质。同时，书中还配有大量案例与源码，帮助读者切实体会深度学习的核心思想和精妙之处。除此之外，书中还介绍了深度学习在视觉领域的应用，从原理层面揭示其思路思想，帮助读者在此领域中夯实技术基础。
《深度学习轻松学：核心算法与视觉实践》十分适合对深度学习感兴趣，希望对深层模型有较深入了解的读者阅读。
                                        </p>]<p><h2>内容简介</h2></p>[<p>适读人群：《深度学习轻松学：核心算法与视觉实践》适合对深度学习感兴趣的读者阅读，也适合有志于从事计算机视觉研究等领域的广大学生阅读，可作为深度学习的入门教材。
本书特色
深入剖析卷积神经网络核心：全连接层和卷积层
深入分析Caffe源码实现架构，了解框架背后的运行机理
详尽介绍网络结构与训练细节，解密复杂运算的基本原理
经典实践场景：图像语意分割，图像生成。GAN模型的详细分析与推导
样例代码采用C  和Python两种语言编写
语言轻松幽默易于理解，特别适合初学者快速掌握深度学习核心思想
 </p>]<p><h2>作者简介</h2></p>[<p>冯超，毕业于中国科学院大学，猿辅导研究团队视觉研究负责人，小猿搜题拍照搜题负责人之一。自2016 年起在知乎开设了自己的专栏——《无痛的机器学习》，发表机器学习与深度学习相关文章，文章以轻松幽默的语言、细致深入的分析为特点，收到了不错的反响，被多家媒体转载。曾多次参与社区技术分享活动。</p>]<p><h2>目录</h2></p>
    1 机器学习与深度学习的概念1
1.1 什么是机器学习 1
1.1.1 机器学习的形式. 2
1.1.2 机器学习的几个组成部分. 8
1.2 深度学习的逆袭 9
1.3 深层模型在视觉领域的应用. 13
1.4 本书的主要内容 15
1.5 总结. 17
2 数学与机器学习基础18
2.1 线性代数基础. 18
2.2 对称矩阵的性质 22
2.2.1 特征值与特征向量 22
2.2.2 对称矩阵的特征值和特征向量 23
2.2.3 对称矩阵的对角化 24
2.3 概率论. 25
2.3.1 概率与分布. 25
2.3.2 *大似然估计 28
2.4 信息论基础 31
2.5 KL 散度. 33
2.6 凸函数及其性质 37
2.7 机器学习基本概念. 39
2.8 机器学习的目标函数 42
2.9 总结. 44
3 CNN 的基石：全连接层45
3.1 线性部分. 45
3.2 非线性部分 48
3.3 神经网络的模样 50
3.4 反向传播法 55
3.4.1 反向传播法的计算方法. 55
3.4.2 反向传播法在计算上的抽象. 58
3.4.3 反向传播法在批量数据上的推广. 59
3.4.4 具体的例子. 63
3.5 参数初始化 65
3.6 总结. 68
4 CNN 的基石：卷积层69
4.1 卷积操作. 69
4.1.1 卷积是什么. 69
4.1.2 卷积层效果展示. 73
4.1.3 卷积层汇总了什么 76
4.1.4 卷积的另一种解释 77
4.2 卷积层的反向传播. 79
4.2.1 实力派解法. 80
4.2.2 “偶像派”解法. 84
4.3 ReLU 88
4.3.1 梯度消失问题 89
4.3.2 ReLU 的理论支撑. 92
4.3.3 ReLU 的线性性质. 93
4.3.4 ReLU 的不足. 93
4.4 总结. 94
4.5 参考文献. 94
5 Caffe 入门95
5.1 使用Caffe 进行深度学习训练. 96
5.1.1 数据预处理. 96
5.1.2 网络结构与模型训练的配置. 100
5.1.3 训练与再训练 108
5.1.4 训练日志分析 110
5.1.5 预测检验与分析. 112
5.1.6 性能测试 115
5.2 模型配置文件介绍. 117
5.3 Caffe 的整体结构. 122
5.3.1 SyncedMemory 124
5.3.2 Blob 125
5.3.3 Layer 125
5.3.4 Net 126
5.3.5 Solver 126
5.3.6 多GPU 训练. 127
5.3.7 IO 127
5.4 Caffe 的Layer 128
5.4.1 Layer 的创建——LayerRegistry
128
5.4.2 Layer 的初始化. 130
5.4.3 Layer 的前向计算. 132
5.5 Caffe 的Net 组装流程 133
5.6 Caffe 的Solver 计算流程. 139
5.6.1 优化流程 140
5.6.2 多卡优化算法 142
5.7 Caffe 的Data Layer 145
5.7.1 Datum 结构. 145
5.7.2 DataReader Thread 147
5.7.3 BasePrefetchingDataLayer Thread 148
5.7.4 Data Layer 149
5.8 Caffe 的Data
Transformer 150
5.8.1 C   中的Data
Transformer 150
5.8.2 Python 中的Data
Transformer 153
5.9 模型层扩展实践——Center Loss
Layer 156
5.9.1 Center Loss 的原理 156
5.9.2 Center Loss 实现. 160
5.9.3 实验分析与总结. 164
5.10 总结. 165
5.11 参考文献. 165
6 深层网络的数值问题166
6.1 ReLU 和参数初始化. 166
6.1.1 **个ReLU 数值实验. 167
6.1.2 第二个ReLU 数值实验. 169
6.1.3 第三个实验——Sigmoid 171
6.2 Xavier 初始化. 172
6.3 MSRA 初始化. 178
6.3.1 前向推导 178
6.3.2 后向推导 181
6.4 ZCA 182
6.5 与数值溢出的战斗. 186
6.5.1 Softmax Layer 186
6.5.2 Sigmoid Cross Entropy Loss 189
6.6 总结. 192
6.7 参考文献. 192
7 网络结构193
7.1 关于网络结构，我们更关心什么 193
7.2 网络结构的演化 195
7.2.1 VGG：模型哲学. 195
7.2.2 GoogLeNet：丰富模型层的内部结构. 196
7.2.3 ResNet：从乘法模型到加法模型. 197
7.2.4 全连接层的没落. 198
7.3 Batch Normalization 199
7.3.1 Normalization 199
7.3.2 使用BN 层的实验. 200
7.3.3 BN 的实现. 201
7.4 对Dropout 的思考. 204
7.5 从迁移学习的角度观察网络功能 206
7.6 ResNet 的深入分析. 210
7.6.1 DSN 解决梯度消失问题 211
7.6.2 ResNet 网络的展开结构. 212
7.6.3 FractalNet 214
7.6.4 DenseNet 215
7.7 总结. 217
7.8 参考文献. 217
8 优化与训练219
8.1 梯度下降是一门手艺活儿. 219
8.1.1 什么是梯度下降法 219
8.1.2 优雅的步长. 220
8.2 路遥知马力：动量. 225
8.3 SGD 的变种算法 232
8.3.1 非凸函数 232
8.3.2 经典算法的弯道表现. 233
8.3.3 Adagrad 234
8.3.4 Rmsprop 235
8.3.5 AdaDelta 236
8.3.6 Adam 237
8.3.7 爬坡赛. 240
8.3.8 总结. 242
8.4 L1 正则的效果. 243
8.4.1 MNIST 的L1 正则实验. 244
8.4.2 次梯度下降法 246
8.5 寻找模型的弱点 251
8.5.1 泛化性实验. 252
8.5.2 精确性实验. 255
8.6 模型优化路径的可视化. 255
8.7 模型的过拟合. 260
8.7.1 过拟合方案. 261
8.7.2 SGD 与过拟合 263
8.7.3 对于深层模型泛化的猜想. 264
8.8 总结. 265
8.9 参考文献. 265
9 应用：图像的语意分割267
9.1 FCN 268
9.2 CRF 通俗非严谨的入门. 272
9.2.1 有向图与无向图模型. 272
9.2.2 Log-Linear Model 278
9.2.3 条件随机场. 280
9.3 Dense CRF 281
9.3.1 Dense CRF 是如何被演化出来的. 281
9.3.2 Dense CRF 的公式形式. 284
9.4 Mean Field 对Dense CRF 模型的化简 285
9.5 Dense CRF 的推断计算公式 288
9.5.1 Variational Inference 推导 289
9.5.2 进一步化简. 291
9.6 完整的模型：CRF as RNN 292
9.7 总结. 294
9.8 参考文献. 294
10 应用：图像生成295
10.1 VAE 295
10.1.1 生成式模型. 295
10.1.2 Variational Lower bound 296
10.1.3 Reparameterization Trick 298
10.1.4 Encoder 和Decoder 的计算公式. 299
10.1.5 实现. 300
10.1.6 MNIST 生成模型可视化 301
10.2 GAN 303
10.2.1 GAN 的概念. 303
10.2.2 GAN 的训练分析. 305
10.2.3 GAN 实战. 309
10.3 Info-GAN 314
10.3.1 互信息. 315
10.3.2 InfoGAN 模型 317
10.4 Wasserstein GAN 320
10.4.1 分布的重叠度 321
10.4.2 两种目标函数存在的问题. 323
10.4.3 Wasserstein 距离. 325
10.4.4 Wasserstein 距离的优势. 329
10.4.5 Wasserstein GAN 的实现 331
10.5 总结. 333
10.6 参考文献. 334
